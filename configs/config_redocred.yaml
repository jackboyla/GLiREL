# Learning Rate
lr_encoder: 1e-5
lr_others: 1e-4
weight_decay_encoder: 0.01
weight_decay_other: 0.01

# Training Parameters
num_steps: 40000
warmup_ratio: 0.1
train_batch_size: 2
eval_every: 500
gradient_accumulation: 2  # null
eval_batch_size: 16
num_layers_freeze: null
early_stopping_patience: 12
early_stopping_delta: 0.0
threshold_search_metric: "macro_f1"
max_saves: 1

# Model Configuration
max_width: 12
model_name: microsoft/deberta-v3-large # hugging face model
fine_tune: true
subtoken_pooling: first
hidden_size: 768
scorer: "dot"
rel_mode: marker
span_marker_mode: markerv1
refine_prompt: false
refine_relation: false
ffn_mul: 4
dropout: 0.4
scheduler: "cosine_with_warmup"  # "cosine_with_hard_restarts"
loss_func: "binary_cross_entropy_loss" # "binary_cross_entropy_loss"  # "focal_loss"
alpha: 0.25
gamma: 2
# add_entity_markers: true
label_embed_strategy: "both"  # "label" "ent_token" "both"

# Coreference Resolution
coref_classifier: false
coref_loss_weight: 10.0
coreference_label: "SELF"

# Directory Paths
dataset_name: "redocred"
root_dir: ablation_backbone
train_data: "data/redocred_train.jsonl"
eval_data: "data/redocred_dev.jsonl"

# "none" if no pretrained model 
prev_path: "logs/zero_rel/zero_rel-2024-10-06__21-28-09/saved_at/model_70000"


# Training Specifics
size_sup: -1
num_train_rel_types: 15   # number of relation labels to use in each given mini-batch
top_k: 1                  # number of relations predictions to return at evaluation time
random_drop: true        # randomly drop relation types
max_len: 512
eval_threshold: [0.001, 0.01, 0.1, 0.2, 0.3, 0.5]
max_entity_pair_distance: 20
# fixed_relation_types: true  # for eval: only use the given list of relation types (FewRel) for all batches
fixed_relation_types: false   # for eval: use all relation types of a given batch (DocRED)
num_unseen_rel_types: 15      # for eval: will be ignored if `fixed_relation_types` is false

name: "large"


