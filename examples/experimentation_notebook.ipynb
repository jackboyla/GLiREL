{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from flair.data import Sentence\n",
    "\n",
    "def flair_sentence(tokens):\n",
    "\n",
    "    sentences = [Sentence(i) for i in tokens]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence[9]: \"The quick brown fox jumps over the lazy dog\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_sentence([['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesusherrera/miniconda3/envs/glirel/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from glirel.modules.token_rep import TokenRepLayer\n",
    "\n",
    "token_rep = TokenRepLayer(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        fine_tune=True,\n",
    "        subtoken_pooling=\"first\",\n",
    "        hidden_size=768,\n",
    "        add_tokens=[],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesusherrera/miniconda3/envs/glirel/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "token_rep = TokenRepLayer(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        fine_tune=True,\n",
    "        subtoken_pooling=\"first\",\n",
    "        hidden_size=768,\n",
    "        add_tokens=[\"[REL]\", \"[SEP]\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_batch = [\n",
    "    [\"Hello\", \"world\"],                 \n",
    "    [\"This\", \"is\", \"a\", \"test\"],        \n",
    "]\n",
    "lengths = torch.tensor([len(seq) for seq in tokens_batch])  \n",
    "\n",
    "output = token_rep(tokens_batch, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.8532, -0.1729, -0.1481,  ...,  0.2372,  0.0825, -0.3372],\n",
       "          [ 1.0145, -0.7746, -0.5060,  ..., -0.0447, -0.0466, -0.3296],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.8800, -0.1707, -0.3937,  ...,  0.0134,  0.2015,  0.1256],\n",
       "          [-1.9091, -0.0522,  0.2174,  ..., -0.0082,  0.3590,  0.0646],\n",
       "          [-0.1139,  0.1377,  0.2996,  ...,  0.1373,  0.6843,  0.0982],\n",
       "          [ 0.6009, -0.5112, -0.0265,  ...,  0.5871,  0.5133, -0.2278]]]),\n",
       " 'mask': tensor([[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "########################################################################\n",
    "# Replicates Flairâ€™s logic\n",
    "########################################################################\n",
    "\n",
    "def fill_masked_elements(all_token_embeddings, hidden_states, mask, word_ids, lengths):\n",
    "    \"\"\"\n",
    "    For 'first' or 'last' subtoken pooling: copy out exactly the subtoken embeddings\n",
    "    that match the mask + are valid word_ids, and place them in the correct positions.\n",
    "    \"\"\"\n",
    "    batch_size = all_token_embeddings.size(0)\n",
    "    for i in range(batch_size):\n",
    "        keep = hidden_states[i][mask[i] & (word_ids[i] >= 0)]\n",
    "        replaced = insert_missing_embeddings(keep, word_ids[i], lengths[i])\n",
    "        all_token_embeddings[i, : lengths[i], :] = replaced\n",
    "    return all_token_embeddings\n",
    "\n",
    "\n",
    "def insert_missing_embeddings(token_embeddings, word_ids_i, length_i):\n",
    "    \"\"\"\n",
    "    If some token indices [0..length_i-1] never appeared in 'token_embeddings',\n",
    "    insert zero-vectors at those positions.\n",
    "    \"\"\"\n",
    "    if token_embeddings.size(0) == 0:\n",
    "        # No subtokens found at all, so fill with zeros\n",
    "        return torch.zeros(\n",
    "            int(length_i),\n",
    "            token_embeddings.size(-1),\n",
    "            device=token_embeddings.device,\n",
    "            dtype=token_embeddings.dtype\n",
    "        )\n",
    "    elif token_embeddings.size(0) < length_i:\n",
    "        # Potentially insert zero-vectors for any missing token positions\n",
    "        for idx in range(int(length_i)):\n",
    "            if not (word_ids_i == idx).any():\n",
    "                zero_vec = torch.zeros_like(token_embeddings[:1])\n",
    "                token_embeddings = torch.cat(\n",
    "                    (token_embeddings[:idx], zero_vec, token_embeddings[idx:]),\n",
    "                    dim=0\n",
    "                )\n",
    "    return token_embeddings\n",
    "\n",
    "\n",
    "def fill_mean_token_embeddings(all_token_embeddings, hidden_states, word_ids, token_lengths):\n",
    "    \"\"\"\n",
    "    For 'mean' subtoken pooling: sum all subtoken embeddings for each token ID\n",
    "    and divide by the subtoken count.\n",
    "    \"\"\"\n",
    "    bsz, max_tokens, emb_dim = all_token_embeddings.shape\n",
    "    # mask to ignore special tokens (CLS, SEP, or None)\n",
    "    mask = (word_ids >= 0)\n",
    "\n",
    "    # sum embeddings for each (batch, token_id)\n",
    "    all_token_embeddings.scatter_add_(\n",
    "        dim=1,\n",
    "        index=word_ids.clamp(min=0).unsqueeze(-1).expand(-1, -1, emb_dim),\n",
    "        src=hidden_states * mask.unsqueeze(-1).float(),\n",
    "    )\n",
    "\n",
    "    # count how many subtokens contributed per token\n",
    "    subtoken_counts = torch.zeros_like(all_token_embeddings[:, :, 0])\n",
    "    subtoken_counts.scatter_add_(\n",
    "        1,\n",
    "        word_ids.clamp(min=0),\n",
    "        mask.float()\n",
    "    )\n",
    "\n",
    "    # average\n",
    "    all_token_embeddings = torch.where(\n",
    "        subtoken_counts.unsqueeze(-1) > 0,\n",
    "        all_token_embeddings / subtoken_counts.unsqueeze(-1),\n",
    "        torch.zeros_like(all_token_embeddings),\n",
    "    )\n",
    "\n",
    "    # zero out positions beyond the actual token length\n",
    "    max_len = max_tokens\n",
    "    idx_range = torch.arange(max_len, device=token_lengths.device).unsqueeze(0)\n",
    "    valid_mask = (idx_range < token_lengths.unsqueeze(1))\n",
    "    all_token_embeddings = all_token_embeddings * valid_mask.unsqueeze(-1)\n",
    "\n",
    "    return all_token_embeddings\n",
    "\n",
    "class CustomTransformerWordEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    A drop-in replacement for Flair's `TransformerWordEmbeddings`:\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, fine_tune: bool, subtoken_pooling: str, allow_long_sentences: bool = True):\n",
    "        \"\"\"\n",
    "        :param model_name: Hugging Face model ID or path\n",
    "        :param fine_tune: Whether to keep the model parameters trainable\n",
    "        :param subtoken_pooling: 'first', 'last', 'mean', or 'first_last'\n",
    "        :param allow_long_sentences: If True, we won't chunk or break up long inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = f\"CustomTransformerWordEmbeddings({model_name})\"\n",
    "        self.model_name = model_name\n",
    "        self.fine_tune = fine_tune\n",
    "        self.subtoken_pooling = subtoken_pooling\n",
    "        self.allow_long_sentences = allow_long_sentences\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Freeze or unfreeze\n",
    "        if not fine_tune:\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # If we do 'first_last', dimension doubles\n",
    "        hidden_size = self.model.config.hidden_size\n",
    "        if subtoken_pooling == \"first_last\":\n",
    "            self._embedding_length = hidden_size * 2\n",
    "        else:\n",
    "            self._embedding_length = hidden_size\n",
    "\n",
    "        # We'll store embeddings under this `self.name`\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self._embedding_length\n",
    "\n",
    "    def embed(self, sentences):\n",
    "        \"\"\"\n",
    "        Expects a list of \"sentence-like\" objects.\n",
    "        Each \"sentence\" must have a `.tokens` list.\n",
    "        Each \"token\" must have:\n",
    "          - a .text attribute\n",
    "          - a .set_embedding(name, vector) method\n",
    "        \"\"\"\n",
    "        if not sentences:\n",
    "            return\n",
    "\n",
    "        # Prepare input: list of list-of-strings\n",
    "        batch_of_lists = []\n",
    "        for s in sentences:\n",
    "            # s.tokens is a list of tokens, each with .text\n",
    "            batch_of_lists.append([t.text for t in s.tokens])\n",
    "\n",
    "        # Tokenize using HF\n",
    "        encoding = self.tokenizer(\n",
    "            batch_of_lists,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=not self.allow_long_sentences\n",
    "        )\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        device = next(self.model.parameters()).device  # move to same device as model\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # shape = [batch_size, seq_len, hidden_dim]\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "\n",
    "        batch_size, seq_len, hidden_dim = last_hidden.shape\n",
    "\n",
    "        # Reconstruct which subtoken belongs to which token\n",
    "        # word_ids(i) => a list of length seq_len with an integer or None\n",
    "        word_ids_batch = []\n",
    "        max_token_count = 0\n",
    "        for i in range(batch_size):\n",
    "            w_ids = encoding.word_ids(batch_index=i)\n",
    "            if w_ids is None:\n",
    "                # fallback (slow tokenizer) => all None\n",
    "                w_ids = [None]*seq_len\n",
    "            # figure out how many tokens are in that sample\n",
    "            valid_ids = [x for x in w_ids if x is not None]\n",
    "            if valid_ids:\n",
    "                max_id = max(valid_ids)\n",
    "                token_count = max_id + 1\n",
    "            else:\n",
    "                token_count = 0\n",
    "            if token_count > max_token_count:\n",
    "                max_token_count = token_count\n",
    "            word_ids_batch.append(w_ids)\n",
    "\n",
    "        # Build a [batch_size, seq_len] tensor of token IDs, or -100 if None\n",
    "        word_ids_tensor = torch.full((batch_size, seq_len), -100, dtype=torch.long, device=device)\n",
    "        for i in range(batch_size):\n",
    "            for j, w_id in enumerate(word_ids_batch[i]):\n",
    "                if w_id is not None:\n",
    "                    word_ids_tensor[i, j] = w_id\n",
    "\n",
    "        # Token lengths per sentence\n",
    "        token_lengths = []\n",
    "        for i in range(batch_size):\n",
    "            valid = [x for x in word_ids_batch[i] if x is not None]\n",
    "            token_lengths.append((max(valid)+1) if valid else 0)\n",
    "        token_lengths_tensor = torch.tensor(token_lengths, device=device, dtype=torch.long)\n",
    "\n",
    "        # Prepare final [batch_size, max_token_count, embedding_dim]\n",
    "        embed_dim = self.embedding_length\n",
    "        all_token_embeddings = torch.zeros(\n",
    "            (batch_size, max_token_count, embed_dim),\n",
    "            device=device, dtype=last_hidden.dtype\n",
    "        )\n",
    "\n",
    "        # Subtoken pooling\n",
    "        if self.subtoken_pooling == \"first\":\n",
    "            # 'first' subtoken => mask out the beginning of each word\n",
    "            gain_mask = (word_ids_tensor[:, 1:] != word_ids_tensor[:, :-1])\n",
    "            # first position is always True\n",
    "            true_tensor = torch.ones((batch_size, 1), dtype=torch.bool, device=device)\n",
    "            first_mask = torch.cat([true_tensor, gain_mask], dim=1)\n",
    "            fill_masked_elements(all_token_embeddings, last_hidden, first_mask, word_ids_tensor, token_lengths_tensor)\n",
    "\n",
    "        elif self.subtoken_pooling == \"last\":\n",
    "            # 'last' subtoken => mask out the boundary at the next subtoken\n",
    "            gain_mask = (word_ids_tensor[:, 1:] != word_ids_tensor[:, :-1])\n",
    "            true_end = torch.ones((batch_size, 1), dtype=torch.bool, device=device)\n",
    "            last_mask = torch.cat([gain_mask, true_end], dim=1)\n",
    "            fill_masked_elements(all_token_embeddings, last_hidden, last_mask, word_ids_tensor, token_lengths_tensor)\n",
    "\n",
    "        elif self.subtoken_pooling == \"first_last\":\n",
    "            # doubling hidden size => first half for 'first', second half for 'last'\n",
    "            real_hsize = self.model.config.hidden_size\n",
    "            gain_mask = (word_ids_tensor[:, 1:] != word_ids_tensor[:, :-1])\n",
    "            ones = torch.ones((batch_size, 1), dtype=torch.bool, device=device)\n",
    "            first_mask = torch.cat([ones, gain_mask], dim=1)\n",
    "            last_mask = torch.cat([gain_mask, ones], dim=1)\n",
    "            # fill first half\n",
    "            fill_masked_elements(\n",
    "                all_token_embeddings[:, :, :real_hsize],\n",
    "                last_hidden, first_mask, word_ids_tensor, token_lengths_tensor\n",
    "            )\n",
    "            # fill second half\n",
    "            fill_masked_elements(\n",
    "                all_token_embeddings[:, :, real_hsize:],\n",
    "                last_hidden, last_mask, word_ids_tensor, token_lengths_tensor\n",
    "            )\n",
    "\n",
    "        elif self.subtoken_pooling == \"mean\":\n",
    "            fill_mean_token_embeddings(all_token_embeddings, last_hidden, word_ids_tensor, token_lengths_tensor)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown subtoken_pooling={self.subtoken_pooling}\")\n",
    "\n",
    "        # Now store each token's embedding\n",
    "        # For each sample i\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            length_i = token_lengths[i]\n",
    "            # slice out the relevant portion\n",
    "            embs_i = all_token_embeddings[i, :length_i]  # shape [length_i, embed_dim]\n",
    "            # set embedding on each token\n",
    "            for token_idx, token in enumerate(sentence.tokens):\n",
    "                token.set_embedding(self.name, embs_i[token_idx])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ModifiedTokenRepLayer(nn.Module):\n",
    "    def __init__(self, model_name: str, fine_tune: bool, subtoken_pooling: str,\n",
    "                 hidden_size: int, add_tokens: List[str]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_layer = CustomTransformerWordEmbeddings(\n",
    "            model_name,\n",
    "            fine_tune=fine_tune,\n",
    "            subtoken_pooling=subtoken_pooling,\n",
    "            allow_long_sentences=True\n",
    "        )\n",
    "\n",
    "        # Add tokens to vocabulary\n",
    "        self.bert_layer.tokenizer.add_tokens(add_tokens)\n",
    "\n",
    "        # Resize token embeddings\n",
    "        self.bert_layer.model.resize_token_embeddings(len(self.bert_layer.tokenizer))\n",
    "\n",
    "        bert_hidden_size = self.bert_layer.embedding_length\n",
    "\n",
    "        if hidden_size != bert_hidden_size:\n",
    "            self.projection = nn.Linear(bert_hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, tokens: List[List[str]], lengths: torch.Tensor):\n",
    "        token_embeddings = self.compute_word_embedding(tokens)\n",
    "\n",
    "        if hasattr(self, \"projection\"):\n",
    "            token_embeddings = self.projection(token_embeddings)\n",
    "\n",
    "        B = len(lengths)\n",
    "        max_length = lengths.max()\n",
    "        mask = (torch.arange(max_length).view(1, -1).repeat(B, 1)\n",
    "                < lengths.cpu().unsqueeze(1)).to(token_embeddings.device).long()\n",
    "\n",
    "        return {\"embeddings\": token_embeddings, \"mask\": mask}\n",
    "\n",
    "    def compute_word_embedding(self, tokens):\n",
    "        # sentences = [Sentence(i) for i in tokens]\n",
    "        # self.bert_layer.embed(sentences)\n",
    "        # we just replicate that approach, but we need minimal \"Sentence\" and \"Token\" classes:\n",
    "\n",
    "        sentences = [MinimalSentence(toks) for toks in tokens]   # see definition below\n",
    "        #sentences = [Sentence(i) for i in tokens]\n",
    "        self.bert_layer.embed(sentences)\n",
    "        # gather embeddings\n",
    "        token_embeddings = pad_sequence(\n",
    "            [torch.stack([tok.get_embedding(self.bert_layer.name) for tok in s.tokens])\n",
    "             for s in sentences],\n",
    "            batch_first=True\n",
    "        )\n",
    "        return token_embeddings\n",
    "\n",
    "class MinimalToken:\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text\n",
    "        self._embeddings = {}\n",
    "\n",
    "    def set_embedding(self, name: str, vector: torch.Tensor):\n",
    "        self._embeddings[name] = vector\n",
    "\n",
    "    def get_embedding(self, name: str) -> torch.Tensor:\n",
    "        return self._embeddings[name]\n",
    "\n",
    "class MinimalSentence:\n",
    "    def __init__(self, list_of_words: List[str]):\n",
    "        self.tokens = [MinimalToken(w) for w in list_of_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesusherrera/miniconda3/envs/glirel/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "modified_token_rep = ModifiedTokenRepLayer(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        fine_tune=True,\n",
    "        subtoken_pooling=\"first\",\n",
    "        hidden_size=768,\n",
    "        add_tokens=[\"[REL]\", \"[SEP]\"],\n",
    "    )\n",
    "    \n",
    "tokens_batch = [\n",
    "    [\"Hello\", \"world\"],                 \n",
    "    [\"This\", \"is\", \"a\", \"test\"],        \n",
    "]\n",
    "lengths = torch.tensor([len(seq) for seq in tokens_batch])  \n",
    "\n",
    "output = modified_token_rep(tokens_batch, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.8532, -0.1729, -0.1481,  ...,  0.2372,  0.0825, -0.3372],\n",
       "          [ 1.0145, -0.7746, -0.5060,  ..., -0.0447, -0.0466, -0.3296],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.8800, -0.1707, -0.3937,  ...,  0.0134,  0.2015,  0.1256],\n",
       "          [-1.9091, -0.0522,  0.2174,  ..., -0.0082,  0.3590,  0.0646],\n",
       "          [-0.1139,  0.1377,  0.2996,  ...,  0.1373,  0.6843,  0.0982],\n",
       "          [ 0.6009, -0.5112, -0.0265,  ...,  0.5871,  0.5133, -0.2278]]],\n",
       "        grad_fn=<CopySlices>),\n",
       " 'mask': tensor([[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesusherrera/miniconda3/envs/glirel/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "token_rep = TokenRepLayer(\n",
    "        model_name=\"microsoft/deberta-v3-small\",\n",
    "        fine_tune=True,\n",
    "        subtoken_pooling=\"first\",\n",
    "        hidden_size=768,\n",
    "        add_tokens=[\"[REL]\", \"[SEP]\"],\n",
    "    )\n",
    "    \n",
    "tokens_batch = [\n",
    "    [\"Hello\", \"world\"],                 \n",
    "    [\"This\", \"is\", \"a\", \"test\"],        \n",
    "]\n",
    "lengths = torch.tensor([len(seq) for seq in tokens_batch])  \n",
    "\n",
    "original_output = token_rep(tokens_batch, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.8532, -0.1729, -0.1481,  ...,  0.2372,  0.0825, -0.3372],\n",
       "          [ 1.0145, -0.7746, -0.5060,  ..., -0.0447, -0.0466, -0.3296],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-1.8800, -0.1707, -0.3937,  ...,  0.0134,  0.2015,  0.1256],\n",
       "          [-1.9091, -0.0522,  0.2174,  ..., -0.0082,  0.3590,  0.0646],\n",
       "          [-0.1139,  0.1377,  0.2996,  ...,  0.1373,  0.6843,  0.0982],\n",
       "          [ 0.6009, -0.5112, -0.0265,  ...,  0.5871,  0.5133, -0.2278]]]),\n",
       " 'mask': tensor([[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output[\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8532, -0.1729, -0.1481,  ...,  0.2372,  0.0825, -0.3372],\n",
       "         [ 1.0145, -0.7746, -0.5060,  ..., -0.0447, -0.0466, -0.3296],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.8800, -0.1707, -0.3937,  ...,  0.0134,  0.2015,  0.1256],\n",
       "         [-1.9091, -0.0522,  0.2174,  ..., -0.0082,  0.3590,  0.0646],\n",
       "         [-0.1139,  0.1377,  0.2996,  ...,  0.1373,  0.6843,  0.0982],\n",
       "         [ 0.6009, -0.5112, -0.0265,  ...,  0.5871,  0.5133, -0.2278]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(original_output[\"embeddings\"], output[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(original_output[\"embeddings\"], output[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "fine_tune = True\n",
    "subtoken_pooling = \"first\"\n",
    "hidden_size = 768\n",
    "add_tokens = [\"[REL]\", \"[SEP]\"]\n",
    "\n",
    "modified_token_rep = ModifiedTokenRepLayer(\n",
    "        model_name=model_name,\n",
    "        fine_tune=fine_tune,\n",
    "        subtoken_pooling=subtoken_pooling,\n",
    "        hidden_size=hidden_size,\n",
    "        add_tokens=add_tokens,\n",
    "    )   \n",
    "\n",
    "token_rep = TokenRepLayer(\n",
    "        model_name=model_name,\n",
    "        fine_tune=fine_tune,\n",
    "        subtoken_pooling=subtoken_pooling,\n",
    "        hidden_size=hidden_size,\n",
    "        add_tokens=add_tokens,\n",
    "    )\n",
    "    \n",
    "tokens_batch = [\n",
    "    [\"Hello\", \"world\"],                 \n",
    "    [\"This\", \"is\", \"a\", \"test\"],        \n",
    "]\n",
    "lengths = torch.tensor([len(seq) for seq in tokens_batch])  \n",
    "\n",
    "output = modified_token_rep(tokens_batch, lengths)\n",
    "original_output = token_rep(tokens_batch, lengths)\n",
    "\n",
    "print(torch.equal(original_output[\"embeddings\"], output[\"embeddings\"]))\n",
    "print(torch.allclose(original_output[\"embeddings\"], output[\"embeddings\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First_last subtoken pooling is failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-7.4865e-01,  2.2039e-01,  4.4916e-01,  3.0461e-02,  4.6049e-01,\n",
       "           -5.8071e-01,  3.2968e-01,  8.3875e-01],\n",
       "          [ 3.8578e-02,  2.7650e-01,  1.8024e-01,  6.8406e-02,  3.4463e-01,\n",
       "           -3.5767e-01,  1.0805e+00,  2.8995e-01],\n",
       "          [ 1.1109e-02,  1.9507e-02, -5.8411e-03,  2.1758e-02, -3.4151e-02,\n",
       "            7.7907e-03, -2.6146e-02,  3.5306e-02],\n",
       "          [ 1.1109e-02,  1.9507e-02, -5.8411e-03,  2.1758e-02, -3.4151e-02,\n",
       "            7.7907e-03, -2.6146e-02,  3.5306e-02]],\n",
       " \n",
       "         [[-1.5341e-01,  2.5130e-01, -4.9714e-01,  3.2337e-01,  3.8418e-02,\n",
       "           -3.8637e-01, -3.1271e-01,  1.1782e+00],\n",
       "          [-6.3128e-01,  7.4822e-02, -6.0581e-01,  5.1159e-01,  1.4507e-01,\n",
       "           -5.4399e-01, -8.3792e-01,  6.5109e-01],\n",
       "          [ 2.0277e-02, -1.7902e-01, -3.0790e-04,  1.2179e-01, -3.9742e-01,\n",
       "           -7.8828e-02, -1.6313e-01,  9.0789e-01],\n",
       "          [-4.3891e-01, -7.5439e-01, -1.9608e-01, -1.2052e-01, -5.9269e-01,\n",
       "            6.7124e-02, -7.8718e-02,  7.4351e-01]]], grad_fn=<ViewBackward0>),\n",
       " 'mask': tensor([[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': tensor([[[-0.9712,  0.2857, -0.1243,  0.7853, -0.3994, -0.3484, -0.2385,\n",
       "            0.6714],\n",
       "          [-0.0792,  0.2863,  0.3726,  1.2731, -0.3714, -0.1627, -0.2779,\n",
       "            0.4744],\n",
       "          [ 0.0170,  0.0078, -0.0107, -0.0303,  0.0333,  0.0278, -0.0318,\n",
       "            0.0191],\n",
       "          [ 0.0170,  0.0078, -0.0107, -0.0303,  0.0333,  0.0278, -0.0318,\n",
       "            0.0191]],\n",
       " \n",
       "         [[-0.8322, -0.1813, -0.2201,  0.7731, -0.4976,  0.2493,  0.2634,\n",
       "           -0.4637],\n",
       "          [-0.5629,  0.0897,  0.3572,  0.6093, -0.4855, -0.2258,  0.2375,\n",
       "            0.2057],\n",
       "          [-0.4416, -0.2267,  0.3529,  0.7161, -0.8860,  0.3869,  0.1543,\n",
       "            0.1356],\n",
       "          [ 0.2068,  0.2609,  0.2046,  0.6199, -1.5394, -0.3388,  0.2332,\n",
       "            0.0019]]], grad_fn=<ViewBackward0>),\n",
       " 'mask': tensor([[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glirel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
